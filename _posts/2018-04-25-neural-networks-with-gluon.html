---
layout: default
title: Neural Networks with Gluon
---
	<div>
		<div class="center">
			<img src="../../../img/gluon.png">
		</div>
		<h1>Neural Networks in Gluon</h1>
		<p>
			I was at <A href="https://aws.amazon.com/new/reinvent/" target="_blank">AWS ReInvent</A> in November, and there
			was a <strong>heavy</strong> emphasis on machine learning at the conference.  Some of the sessions had 45 minute wait lines
			just to be told that there was no room at the inn.  Clearly there is a lot of interest in the sector among early adopters.
		</p>
		<p>
			One of the better sessions I attended was a hands-on session focused on building neural networks with 
			<a href="https://mxnet.incubator.apache.org/tutorials/gluon/gluon.html">Gluon.</a>
		</p>
		<p>
			Directly from the website:
			<em>
				Gluon package is a high-level interface for MXNet designed to be easy to use while keeping most of the flexibility of 
				w level API. 
			</em>
		</p>
		<h2>What It Is</h2>
		<p>
			Gluon provides a very developer-friendly interface for constructing neural networks in python.  The API provides high-level
			abstractions of the underlying concepts in a neural network.  Where in <a href="https://www.tensorflow.org/" target="_blank">other packages</a> 
			you would be concerned with declaring weights and biases (with their associated types) at each level of your network, Gluon 
			provides a nice abstraction that allows you to focus on the higher-level construction of your network as a whole.
		</p>
		<h3>Tensorflow Example</h3>
		<p>
			If we want to construct a simple neural network in tensorflow with a architecture of a convolutional layer, 
			a max pooling layer, a drop out layer, a fully connected layer and a softmax output layer, it would look something like this:
		</p>
		<pre>
			<code>
  
    x = tf.placeholder(tf.float32, [None, INPUT_VECTOR_SIZE])
    y_ = tf.placeholder(tf.float32, [None, LABEL_SIZE])

    #layer 1
    W_conv1 = weight_variable([PATCH_SIZE, PATCH_SIZE, 1, LAYER_1_FEATURES])
    b_conv1 = bias_variable([LAYER_1_FEATURES])

    x_image = tf.reshape(x, [-1, 64, 64, 1])

    conv_calculation1 = conv2d(x_image, W_conv1) + b_conv1
    h_conv1 = tf.nn.relu(conv_calculation1)
    h_pool1 = max_pool_2x2(h_conv1)

    #second layer
    W_conv2 = weight_variable([PATCH_SIZE, PATCH_SIZE, LAYER_1_FEATURES, LAYER_2_FEATURES])
    b_conv2 = bias_variable([LAYER_2_FEATURES])

    conv_calculation2 = conv2d(h_pool1, W_conv2) + b_conv2
    h_pool2 = max_pool_2x2(conv_calculation2)

    W_fc1 = weight_variable([16 * 16 * LAYER_2_FEATURES, INPUT_VECTOR_SIZE])
    b_fc1 = bias_variable([INPUT_VECTOR_SIZE])

    h_pool2_flat = tf.reshape(h_pool2, [-1, 16 * 16 * LAYER_2_FEATURES])
    fc_calculation = tf.matmul(h_pool2_flat, W_fc1) + b_fc1
    h_fc1 = tf.nn.relu(fc_calculation)

    keep_prob = tf.placeholder(tf.float32)
    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)

    W_fc2 = weight_variable([INPUT_VECTOR_SIZE, LABEL_SIZE])
    b_fc2 = bias_variable([LABEL_SIZE])

    y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2

    softmax = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv)
    cross_entropy = tf.reduce_mean(softmax)

    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)

    correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.arg_max(y_, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
			</code>
		</pre>
		<p>
			Now, that's certainly not that bad when it comes to declaring a neural network.  However, when we look at the same network
			declared using the Gluon API, we can see that there are a few striking differences:
		</p>
		<pre>
			<code>
	softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss(sparse_label=False, batch_axis=1)

    full_connect_outputs = num_outputs
	net = gluon.nn.Sequential()

	with net.name_scope():
		net.add(gluon.nn.Conv2D(channels=50, kernel_size=3, strides=1, activation='relu'))
		net.add(gluon.nn.MaxPool2D(pool_size=4, strides=1))
		
		net.add(gluon.nn.Flatten())
		net.add(gluon.nn.Dense(full_connect_outputs, activation='relu'))
		net.add(gluon.nn.Dropout(.5))
		net.add(gluon.nn.Dense(num_outputs))
			</code>
		</pre>
		<p>
			As you can see, it is a bit more declarative than the tensorflow version, and allows you to easily focus
			on the overall design of your network, without worrying about declaring types and calculating input sizes/shapes.
		</p>
		<p>
			Abstraction usually comes at a price, and in this case the price can be that you give up some control over the
			bare-metal declarations of your types and shapes that you are probably used to having in most NN frameworks (
			I personally was fighting a nasty float casting issue the other day, and the exceptions being generated by Gluon 
			were not exactly the most user friendly I have encountered).
		</p>
		<p>
			To sum up, I like the Gluon API for it's declarative approach and ease of use, but as with all things, the trade-offs 
			must be considered if choosing between Gluon and other NN frameworks.
		</p>
	</div>